# Principal Component Analysis & Linear Discriminant Analysis PCA LDA In Python
(40 points) **PCA and LDA**  
In  datasetdataset1.csv,  columns  correspond  to  variables  and  there  are  twovariables namedV1andV2.  
1.  PlotV2vsV1.  Do you see a clear separation of the raw data?  
2.  Apply your own PCA function to this dataset without scaling the two vari-ables.  Project the raw data onto your first principal component axis, i.e.  thePC1  axis.   Do  you  still  see  a  clear  separation  of  the  data  in  PC1,  i.e.   inprojections of your raw data on the PC1 axis?  
3. Add the PC1 axis to the plot you obtained in (1).  
4.  Apply  your  own  LDA  function  to  this  dataset  and  obtainW.   The  classinformation of each data point is in thelabelcolumn.  
5.  Project your raw data ontoW.  Do you see a clear separation of the data inthe projection ontoW?  
6.  Add theWaxis to your plot.  At this point, your plot should contain the rawdata points, the PC1 axis you obtain from the PCA analysis, and theWaxisyou obtain from the LDA analysis.  
7.  Compute  the  variance  of  the  projections  onto  PC1  and  PC2  axes.   Whatis  the  relationship  between  these  two  variances  and  the  eigenvalues  of  thecovariance matrix you use for computing PC1 and PC2 axes?  
8.  Compute the variance of the projections onto the W axis.  
9.  What message can you get from the above PCA and LDA analyses?
